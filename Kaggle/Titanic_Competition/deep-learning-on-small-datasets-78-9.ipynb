{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02003,
     "end_time": "2021-01-14T01:46:58.295726",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.275696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Solving the Titanic surival problem\n",
    "\n",
    "There are many methods to solving the Titanic survival problem, but none of them matter if the data is not analysed thoroughly. The dataset that is given is very small in comparison to other datasets used in machine learning problems, and most would say deep learning is a futile method to use, so extracting the necessary features is the most important step to solving this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017459,
     "end_time": "2021-01-14T01:46:58.331240",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.313781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import basic libraries\n",
    "\n",
    "The first step is to import the libraries used in all machine learning problems, *NumPy* and *Pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.371582Z",
     "iopub.status.busy": "2021-01-14T01:46:58.370760Z",
     "iopub.status.idle": "2021-01-14T01:46:58.373272Z",
     "shell.execute_reply": "2021-01-14T01:46:58.373657Z"
    },
    "papermill": {
     "duration": 0.024991,
     "end_time": "2021-01-14T01:46:58.373807",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.348816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017879,
     "end_time": "2021-01-14T01:46:58.409895",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.392016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the data\n",
    "\n",
    "Next, we need to load the data by using the previously imported *Pandas* library. We load the training and test data into separate variables and then combine them into one. We also save the *'PassengerId'* column of the test set so we can separate the datasets after the feature engineering is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.452627Z",
     "iopub.status.busy": "2021-01-14T01:46:58.452134Z",
     "iopub.status.idle": "2021-01-14T01:46:58.501593Z",
     "shell.execute_reply": "2021-01-14T01:46:58.502079Z"
    },
    "papermill": {
     "duration": 0.074746,
     "end_time": "2021-01-14T01:46:58.502198",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.427452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1       0.0       3   \n",
       "1            2       1.0       1   \n",
       "2            3       1.0       3   \n",
       "3            4       1.0       1   \n",
       "4            5       0.0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input/titanic/train.csv')\n",
    "test_data = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "test_ids = test_data['PassengerId']\n",
    "\n",
    "data = pd.concat([train_data, test_data], axis=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018348,
     "end_time": "2021-01-14T01:46:58.539524",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.521176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "Before getting deep into analysing the data, we will use the info function to get a basic idea of the values that are contained in each of the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.581226Z",
     "iopub.status.busy": "2021-01-14T01:46:58.580534Z",
     "iopub.status.idle": "2021-01-14T01:46:58.592354Z",
     "shell.execute_reply": "2021-01-14T01:46:58.591920Z"
    },
    "papermill": {
     "duration": 0.0345,
     "end_time": "2021-01-14T01:46:58.592436",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.557936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1309 non-null   int64  \n",
      " 1   Survived     891 non-null    float64\n",
      " 2   Pclass       1309 non-null   int64  \n",
      " 3   Name         1309 non-null   object \n",
      " 4   Sex          1309 non-null   object \n",
      " 5   Age          1046 non-null   float64\n",
      " 6   SibSp        1309 non-null   int64  \n",
      " 7   Parch        1309 non-null   int64  \n",
      " 8   Ticket       1309 non-null   object \n",
      " 9   Fare         1308 non-null   float64\n",
      " 10  Cabin        295 non-null    object \n",
      " 11  Embarked     1307 non-null   object \n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 132.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019084,
     "end_time": "2021-01-14T01:46:58.630426",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.611342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can observe that the columns *'Age', 'Cabin'* and *'Embarked'* have some missing values, which means we will have to find a way to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01859,
     "end_time": "2021-01-14T01:46:58.668296",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.649706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop columns that do not carry valuable information\n",
    "\n",
    "The *'Cabin'* and *'Ticket'* columns do not carry any useful information about the survival chance of the passengers, so we can just remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.711914Z",
     "iopub.status.busy": "2021-01-14T01:46:58.711387Z",
     "iopub.status.idle": "2021-01-14T01:46:58.714229Z",
     "shell.execute_reply": "2021-01-14T01:46:58.713784Z"
    },
    "papermill": {
     "duration": 0.02716,
     "end_time": "2021-01-14T01:46:58.714312",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.687152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Cabin', 'Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018961,
     "end_time": "2021-01-14T01:46:58.752376",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.733415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop the passengers that do not have values for 'Embarked' column\n",
    "\n",
    "Seeing as there are only two rows which have NaN values in the *'Embarked'* column, they can be safely removed as they won't affect the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.794064Z",
     "iopub.status.busy": "2021-01-14T01:46:58.793546Z",
     "iopub.status.idle": "2021-01-14T01:46:58.806827Z",
     "shell.execute_reply": "2021-01-14T01:46:58.806402Z"
    },
    "papermill": {
     "duration": 0.034712,
     "end_time": "2021-01-14T01:46:58.806915",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.772203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1307 entries, 0 to 417\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1307 non-null   int64  \n",
      " 1   Survived     889 non-null    float64\n",
      " 2   Pclass       1307 non-null   int64  \n",
      " 3   Name         1307 non-null   object \n",
      " 4   Sex          1307 non-null   object \n",
      " 5   Age          1044 non-null   float64\n",
      " 6   SibSp        1307 non-null   int64  \n",
      " 7   Parch        1307 non-null   int64  \n",
      " 8   Fare         1306 non-null   float64\n",
      " 9   Embarked     1307 non-null   object \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 112.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data = data[data['Embarked'].notna()]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019654,
     "end_time": "2021-01-14T01:46:58.845877",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.826223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extract useful features from the 'Name' column\n",
    "\n",
    "The *'Name'* column in itself is not very telling to the survival chance of the passenger, but some features may be extracted from it that will definitely aid us in our classification problem. From the name of the passenger we can extract the title as well as the surname, which will help in distinguishing family relations between the passengers.\n",
    "\n",
    "Additionally, seeing as the surname is a string variable, we encode it using the *LabelEncoder* function from *sklearn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:58.897797Z",
     "iopub.status.busy": "2021-01-14T01:46:58.897121Z",
     "iopub.status.idle": "2021-01-14T01:46:59.673201Z",
     "shell.execute_reply": "2021-01-14T01:46:59.673601Z"
    },
    "papermill": {
     "duration": 0.808546,
     "end_time": "2021-01-14T01:46:59.673774",
     "exception": false,
     "start_time": "2021-01-14T01:46:58.865228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data['Title'] = [value.split(', ')[1].split('.')[0] for value in data['Name'].values]\n",
    "data.loc[(data['Title'] == 'Lady') | (data['Title'] == 'Mme') | (data['Title'] == 'Ms') | (data['Title'] == 'the Countess') | (data['Title'] == 'Mlle'), 'Title'] = 'Miss'\n",
    "data.loc[(data['Title'] != 'Mr') & (data['Title'] != 'Mrs') & (data['Title'] != 'Miss') & (data['Title'] != 'Master'), 'Title'] = 'Other'\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['Surname'] = encoder.fit_transform([value.split(', ')[0] for value in data['Name'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020179,
     "end_time": "2021-01-14T01:46:59.714549",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.694370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extract the 'Alone' and 'FamilyCount' features\n",
    "\n",
    "From the *'Parch'* and *'SibSp'* features we can extract information about the passenger's companionship aboard the Titanic, specifically whether they are alone, and if they are not, how many family members they have on-board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:59.758214Z",
     "iopub.status.busy": "2021-01-14T01:46:59.757627Z",
     "iopub.status.idle": "2021-01-14T01:46:59.792482Z",
     "shell.execute_reply": "2021-01-14T01:46:59.793195Z"
    },
    "papermill": {
     "duration": 0.058225,
     "end_time": "2021-01-14T01:46:59.793337",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.735112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Surname</th>\n",
       "      <th>Alone</th>\n",
       "      <th>FamilyCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1       0.0       3   \n",
       "1            2       1.0       1   \n",
       "2            3       1.0       3   \n",
       "3            4       1.0       1   \n",
       "4            5       0.0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch     Fare Embarked Title  Surname  Alone  FamilyCount  \n",
       "0      0   7.2500        S    Mr      100      0            1  \n",
       "1      0  71.2833        C   Mrs      182      0            1  \n",
       "2      0   7.9250        S  Miss      329      1            0  \n",
       "3      0  53.1000        S   Mrs      267      0            1  \n",
       "4      0   8.0500        S    Mr       15      1            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Alone'] = np.zeros(data.shape[0], dtype=np.int)\n",
    "data.loc[(data['SibSp'] == 0) & (data['Parch'] == 0), 'Alone'] = 1\n",
    "\n",
    "data['FamilyCount'] = data['SibSp'] + data['Parch']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020323,
     "end_time": "2021-01-14T01:46:59.834500",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.814177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fill missing values in 'Age' column\n",
    "\n",
    "The only feature vector that has NaN values left is the *'Age'* column. We can simply fill the missing values with the mean age value of the entire dataset, but to achieve greater accuracy in the predicted age, we use the mean value for each title. Additionally, for the titles of 'Mr', 'Mrs' and 'Miss' we add a gaussian distribution of numbers to the mean value to further improve the accuracy of the predicted age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:46:59.879557Z",
     "iopub.status.busy": "2021-01-14T01:46:59.878926Z",
     "iopub.status.idle": "2021-01-14T01:46:59.930838Z",
     "shell.execute_reply": "2021-01-14T01:46:59.930336Z"
    },
    "papermill": {
     "duration": 0.07544,
     "end_time": "2021-01-14T01:46:59.930937",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.855497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=157)\n",
    "\n",
    "# Find mean values of 'Age' for each 'Title' variable\n",
    "mean_ages_title = data[['Title', 'Age']].loc[data['Age'].notna()].groupby('Title').mean()\n",
    "age_nan_titles = data['Title'].loc[data['Age'].isnull()].unique()\n",
    "\n",
    "# Fill 'Age' values that are missing with mean value for each 'Title' class\n",
    "for i in range(len(age_nan_titles)):\n",
    "    if age_nan_titles[i] == 'Mr' or age_nan_titles[i] == 'Mrs' or age_nan_titles[i] == 'Miss':\n",
    "        data.loc[(data['Age'].isnull()) & (data['Title'] == age_nan_titles[i]), 'Age'] = [np.ceil(mean_ages_title.loc[age_nan_titles[i]].values[0]) + np.random.randint(-6, 6)\n",
    "                                                                                          for _ in data['Age'].loc[(data['Age'].isnull()) & (data['Title'] == age_nan_titles[i])]]\n",
    "    else:\n",
    "        data.loc[(data['Age'].isnull()) & (data['Title'] == age_nan_titles[i]), 'Age'] = [np.ceil(mean_ages_title.loc[age_nan_titles[i]].values[0])\n",
    "                                                                                          for _ in data['Age'].loc[(data['Age'].isnull()) & (data['Title'] == age_nan_titles[i])]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02063,
     "end_time": "2021-01-14T01:46:59.972625",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.951995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extract 'AgeRange' feature to transform the numerical 'Age' column to categorical\n",
    "\n",
    "Now that the missing values for *'Age'* have been filled, we can transform the numerical feature to a more useful categorical one, by dividing the ages into five distinct groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.031342Z",
     "iopub.status.busy": "2021-01-14T01:47:00.030734Z",
     "iopub.status.idle": "2021-01-14T01:47:00.037628Z",
     "shell.execute_reply": "2021-01-14T01:47:00.037156Z"
    },
    "papermill": {
     "duration": 0.044335,
     "end_time": "2021-01-14T01:47:00.037734",
     "exception": false,
     "start_time": "2021-01-14T01:46:59.993399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, age_categories = pd.cut(data['Age'], 5, retbins=True)\n",
    "age_categories = [np.floor(value) for value in age_categories]\n",
    "\n",
    "data['AgeRange'] = np.zeros(len(data), dtype=np.int)\n",
    "\n",
    "data.loc[data['Age'] <= age_categories[1], 'AgeRange'] = 0\n",
    "data.loc[(data['Age'] > age_categories[1]) & (data['Age'] <= age_categories[2]), 'AgeRange'] = 1\n",
    "data.loc[(data['Age'] > age_categories[2]) & (data['Age'] <= age_categories[3]), 'AgeRange'] = 2\n",
    "data.loc[(data['Age'] > age_categories[3]) & (data['Age'] <= age_categories[4]), 'AgeRange'] = 3\n",
    "data.loc[data['Age'] > age_categories[4], 'AgeRange'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021672,
     "end_time": "2021-01-14T01:47:00.080891",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.059219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extract 'FareRange' feature to transform the numerical 'Fare' column to categorical\n",
    "\n",
    "We use the same procedure as before for the numerical *'Fare'* feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.131267Z",
     "iopub.status.busy": "2021-01-14T01:47:00.130558Z",
     "iopub.status.idle": "2021-01-14T01:47:00.139199Z",
     "shell.execute_reply": "2021-01-14T01:47:00.139697Z"
    },
    "papermill": {
     "duration": 0.037509,
     "end_time": "2021-01-14T01:47:00.139820",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.102311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, fare_categories = pd.qcut(data['Fare'], 4, retbins=True)\n",
    "\n",
    "data['FareRange'] = np.zeros(len(data), dtype=np.int)\n",
    "data.loc[data['Fare'] <= fare_categories[1], 'FareRange'] = 0\n",
    "data.loc[(data['Fare'] > fare_categories[1]) & (data['Fare'] <= fare_categories[2]), 'FareRange'] = 1\n",
    "data.loc[(data['Fare'] > fare_categories[2]) & (data['Fare'] <= fare_categories[3]), 'FareRange'] = 2\n",
    "data.loc[data['Fare'] > fare_categories[3], 'FareRange'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020115,
     "end_time": "2021-01-14T01:47:00.180750",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.160635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "We convert the categorical features into binary ones to achieve a better and more refined classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.228266Z",
     "iopub.status.busy": "2021-01-14T01:47:00.227754Z",
     "iopub.status.idle": "2021-01-14T01:47:00.237906Z",
     "shell.execute_reply": "2021-01-14T01:47:00.237403Z"
    },
    "papermill": {
     "duration": 0.036646,
     "end_time": "2021-01-14T01:47:00.238001",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.201355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data=data, columns=['Sex', 'Embarked', 'Pclass', 'AgeRange', 'FareRange', 'Title'], dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020209,
     "end_time": "2021-01-14T01:47:00.278991",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.258782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Drop columns that are no longer useful\n",
    "\n",
    "For the final step we remove the columns that are no longer of use, and with that the feature engineering of the dataset is finally complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.325328Z",
     "iopub.status.busy": "2021-01-14T01:47:00.324841Z",
     "iopub.status.idle": "2021-01-14T01:47:00.330090Z",
     "shell.execute_reply": "2021-01-14T01:47:00.329569Z"
    },
    "papermill": {
     "duration": 0.030382,
     "end_time": "2021-01-14T01:47:00.330191",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.299809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Name', 'Age', 'Fare', 'SibSp', 'Parch'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020866,
     "end_time": "2021-01-14T01:47:00.371715",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.350849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Separate data into train, validation and test subsets\n",
    "\n",
    "We separate the training and test data again, and further extract a new validation dataset from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.416570Z",
     "iopub.status.busy": "2021-01-14T01:47:00.416084Z",
     "iopub.status.idle": "2021-01-14T01:47:00.490798Z",
     "shell.execute_reply": "2021-01-14T01:47:00.490330Z"
    },
    "papermill": {
     "duration": 0.097833,
     "end_time": "2021-01-14T01:47:00.490894",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.393061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>Alone</th>\n",
       "      <th>FamilyCount</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>...</th>\n",
       "      <th>AgeRange_4</th>\n",
       "      <th>FareRange_0</th>\n",
       "      <th>FareRange_1</th>\n",
       "      <th>FareRange_2</th>\n",
       "      <th>FareRange_3</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>819</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>542</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>641</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>638</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Surname  Alone  FamilyCount  Sex_female  Sex_male  Embarked_C  \\\n",
       "14       819      1            0           1         0           0   \n",
       "709      542      0            2           0         1           1   \n",
       "427      641      1            0           1         0           0   \n",
       "198      473      1            0           1         0           0   \n",
       "442      638      0            1           0         1           0   \n",
       "\n",
       "     Embarked_Q  Embarked_S  Pclass_1  Pclass_2  ...  AgeRange_4  FareRange_0  \\\n",
       "14            0           1         0         0  ...           0            1   \n",
       "709           0           0         0         0  ...           0            0   \n",
       "427           0           1         0         1  ...           0            0   \n",
       "198           1           0         0         0  ...           0            1   \n",
       "442           0           1         0         0  ...           0            1   \n",
       "\n",
       "     FareRange_1  FareRange_2  FareRange_3  Title_Master  Title_Miss  \\\n",
       "14             0            0            0             0           1   \n",
       "709            0            1            0             1           0   \n",
       "427            0            1            0             0           1   \n",
       "198            0            0            0             0           1   \n",
       "442            0            0            0             0           0   \n",
       "\n",
       "     Title_Mr  Title_Mrs  Title_Other  \n",
       "14          0          0            0  \n",
       "709         0          0            0  \n",
       "427         0          0            0  \n",
       "198         0          0            0  \n",
       "442         1          0            0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = data.loc[data['PassengerId'].isin(test_ids) == False]\n",
    "y_train = X_train['Survived']\n",
    "\n",
    "X_test = data.loc[data['PassengerId'].isin(test_ids)]\n",
    "\n",
    "X_train = X_train.drop(['PassengerId', 'Survived'], axis=1)\n",
    "X_test = X_test.drop(['PassengerId', 'Survived'], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=157, stratify=y_train)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022281,
     "end_time": "2021-01-14T01:47:00.535726",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.513445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Standardization\n",
    "\n",
    "Before training the MLP Neural Network, to achieve better results it is advised to standardize the data. Even though most of our data is binary, this step still helps with achieving higher accuracy of the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.590536Z",
     "iopub.status.busy": "2021-01-14T01:47:00.589524Z",
     "iopub.status.idle": "2021-01-14T01:47:00.601357Z",
     "shell.execute_reply": "2021-01-14T01:47:00.601808Z"
    },
    "papermill": {
     "duration": 0.042729,
     "end_time": "2021-01-14T01:47:00.601916",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.559187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "column_names = X_train.columns\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=column_names)\n",
    "\n",
    "column_names = X_val.columns\n",
    "X_val = pd.DataFrame(scaler.fit_transform(X_val), columns=column_names)\n",
    "\n",
    "column_names = X_test.columns\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023055,
     "end_time": "2021-01-14T01:47:00.647577",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.624522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building the Keras MLP neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:00.697922Z",
     "iopub.status.busy": "2021-01-14T01:47:00.697007Z",
     "iopub.status.idle": "2021-01-14T01:47:05.581889Z",
     "shell.execute_reply": "2021-01-14T01:47:05.582341Z"
    },
    "papermill": {
     "duration": 4.910466,
     "end_time": "2021-01-14T01:47:05.582469",
     "exception": false,
     "start_time": "2021-01-14T01:47:00.672003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.activations import tanh\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "init_tanh = glorot_uniform(seed=157)    # used for tanh and softmax\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input and first Dense layer\n",
    "model.add(Dense(units=X_train.shape[1], kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(tanh))\n",
    "\n",
    "# second Dense layer\n",
    "model.add(Dense(units=X_train.shape[1] * 2, kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(tanh))\n",
    "\n",
    "# third Dense layer\n",
    "model.add(Dense(units=X_train.shape[1] * 4, kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(tanh))\n",
    "\n",
    "# fourth Dense layer\n",
    "model.add(Dense(units=X_train.shape[1], kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(tanh))\n",
    "\n",
    "# fifth Dense layer\n",
    "model.add(Dense(units=X_train.shape[1] * 4, kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(tanh))\n",
    "\n",
    "# output Dense layer\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021247,
     "end_time": "2021-01-14T01:47:05.625747",
     "exception": false,
     "start_time": "2021-01-14T01:47:05.604500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setting all random variables to a single seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:05.672884Z",
     "iopub.status.busy": "2021-01-14T01:47:05.672090Z",
     "iopub.status.idle": "2021-01-14T01:47:05.680216Z",
     "shell.execute_reply": "2021-01-14T01:47:05.680663Z"
    },
    "papermill": {
     "duration": 0.032708,
     "end_time": "2021-01-14T01:47:05.680802",
     "exception": false,
     "start_time": "2021-01-14T01:47:05.648094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = str(157)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(157)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(157)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.compat.v1.set_random_seed(157)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from tensorflow.python.keras import backend as k\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                        inter_op_parallelism_threads=1)\n",
    "\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "k.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021976,
     "end_time": "2021-01-14T01:47:05.724765",
     "exception": false,
     "start_time": "2021-01-14T01:47:05.702789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:05.774238Z",
     "iopub.status.busy": "2021-01-14T01:47:05.773424Z",
     "iopub.status.idle": "2021-01-14T01:47:10.158836Z",
     "shell.execute_reply": "2021-01-14T01:47:10.158160Z"
    },
    "papermill": {
     "duration": 4.410636,
     "end_time": "2021-01-14T01:47:10.158942",
     "exception": false,
     "start_time": "2021-01-14T01:47:05.748306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "12/16 [=====================>........] - ETA: 0s - loss: 0.7262 - accuracy: 0.6267\n",
      "Epoch 00001: val_loss improved from inf to 0.47811, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6701 - accuracy: 0.6550 - val_loss: 0.4781 - val_accuracy: 0.8539\n",
      "Epoch 2/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5566 - accuracy: 0.7400\n",
      "Epoch 00002: val_loss improved from 0.47811 to 0.46387, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4859 - accuracy: 0.7725 - val_loss: 0.4639 - val_accuracy: 0.8539\n",
      "Epoch 3/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4855 - accuracy: 0.7200\n",
      "Epoch 00003: val_loss improved from 0.46387 to 0.44295, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4524 - accuracy: 0.7713 - val_loss: 0.4429 - val_accuracy: 0.8427\n",
      "Epoch 4/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4725 - accuracy: 0.7400\n",
      "Epoch 00004: val_loss improved from 0.44295 to 0.41858, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4278 - accuracy: 0.8163 - val_loss: 0.4186 - val_accuracy: 0.8427\n",
      "Epoch 5/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4594 - accuracy: 0.8000\n",
      "Epoch 00005: val_loss did not improve from 0.41858\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8325 - val_loss: 0.4266 - val_accuracy: 0.8539\n",
      "Epoch 6/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5139 - accuracy: 0.7600\n",
      "Epoch 00006: val_loss improved from 0.41858 to 0.39255, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3954 - accuracy: 0.8325 - val_loss: 0.3925 - val_accuracy: 0.8539\n",
      "Epoch 7/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3539 - accuracy: 0.8200\n",
      "Epoch 00007: val_loss improved from 0.39255 to 0.38997, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3870 - accuracy: 0.8425 - val_loss: 0.3900 - val_accuracy: 0.8652\n",
      "Epoch 8/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.5041 - accuracy: 0.7400\n",
      "Epoch 00008: val_loss improved from 0.38997 to 0.35106, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3621 - accuracy: 0.8512 - val_loss: 0.3511 - val_accuracy: 0.8876\n",
      "Epoch 9/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2830 - accuracy: 0.8800\n",
      "Epoch 00009: val_loss did not improve from 0.35106\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8475 - val_loss: 0.3713 - val_accuracy: 0.8652\n",
      "Epoch 10/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.4007 - accuracy: 0.8000\n",
      "Epoch 00010: val_loss improved from 0.35106 to 0.34465, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3521 - accuracy: 0.8575 - val_loss: 0.3446 - val_accuracy: 0.8876\n",
      "Epoch 11/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3630 - accuracy: 0.8800\n",
      "Epoch 00011: val_loss did not improve from 0.34465\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8625 - val_loss: 0.3462 - val_accuracy: 0.8764\n",
      "Epoch 12/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3344 - accuracy: 0.8600\n",
      "Epoch 00012: val_loss improved from 0.34465 to 0.31948, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3341 - accuracy: 0.8637 - val_loss: 0.3195 - val_accuracy: 0.8876\n",
      "Epoch 13/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3930 - accuracy: 0.7800\n",
      "Epoch 00013: val_loss did not improve from 0.31948\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8625 - val_loss: 0.3817 - val_accuracy: 0.8539\n",
      "Epoch 14/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2365 - accuracy: 0.9400\n",
      "Epoch 00014: val_loss improved from 0.31948 to 0.30399, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3323 - accuracy: 0.8600 - val_loss: 0.3040 - val_accuracy: 0.8876\n",
      "Epoch 15/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2304 - accuracy: 0.9000\n",
      "Epoch 00015: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8712 - val_loss: 0.3537 - val_accuracy: 0.8652\n",
      "Epoch 16/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3571 - accuracy: 0.8800\n",
      "Epoch 00016: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8687 - val_loss: 0.3109 - val_accuracy: 0.8652\n",
      "Epoch 17/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2591 - accuracy: 0.8800\n",
      "Epoch 00017: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8587 - val_loss: 0.3256 - val_accuracy: 0.8652\n",
      "Epoch 18/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2816 - accuracy: 0.9000\n",
      "Epoch 00018: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8725 - val_loss: 0.3262 - val_accuracy: 0.8539\n",
      "Epoch 19/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3625 - accuracy: 0.8600\n",
      "Epoch 00019: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8763 - val_loss: 0.3473 - val_accuracy: 0.8652\n",
      "Epoch 20/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2985 - accuracy: 0.9000\n",
      "Epoch 00020: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8788 - val_loss: 0.3493 - val_accuracy: 0.8876\n",
      "Epoch 21/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2236 - accuracy: 0.9400\n",
      "Epoch 00021: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8700 - val_loss: 0.3352 - val_accuracy: 0.8539\n",
      "Epoch 22/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2129 - accuracy: 0.9000\n",
      "Epoch 00022: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.8813 - val_loss: 0.3285 - val_accuracy: 0.8652\n",
      "Epoch 23/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2509 - accuracy: 0.8800\n",
      "Epoch 00023: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8712 - val_loss: 0.3988 - val_accuracy: 0.8090\n",
      "Epoch 24/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3646 - accuracy: 0.8600\n",
      "Epoch 00024: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8925 - val_loss: 0.3119 - val_accuracy: 0.8764\n",
      "Epoch 25/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2675 - accuracy: 0.9400\n",
      "Epoch 00025: val_loss did not improve from 0.30399\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8775 - val_loss: 0.3226 - val_accuracy: 0.8764\n",
      "Epoch 26/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2814 - accuracy: 0.8800\n",
      "Epoch 00026: val_loss improved from 0.30399 to 0.29784, saving model to neural_network_checkpoint_training.h5\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2864 - accuracy: 0.8863 - val_loss: 0.2978 - val_accuracy: 0.8764\n",
      "Epoch 27/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2348 - accuracy: 0.9200\n",
      "Epoch 00027: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8863 - val_loss: 0.3129 - val_accuracy: 0.8652\n",
      "Epoch 28/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3274 - accuracy: 0.9000\n",
      "Epoch 00028: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8850 - val_loss: 0.3842 - val_accuracy: 0.8315\n",
      "Epoch 29/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3213 - accuracy: 0.9000\n",
      "Epoch 00029: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2928 - accuracy: 0.8763 - val_loss: 0.3187 - val_accuracy: 0.8876\n",
      "Epoch 30/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3473 - accuracy: 0.9200\n",
      "Epoch 00030: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8788 - val_loss: 0.3426 - val_accuracy: 0.8876\n",
      "Epoch 31/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2558 - accuracy: 0.9000\n",
      "Epoch 00031: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8850 - val_loss: 0.3214 - val_accuracy: 0.8539\n",
      "Epoch 32/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2648 - accuracy: 0.9000\n",
      "Epoch 00032: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8925 - val_loss: 0.3568 - val_accuracy: 0.8652\n",
      "Epoch 33/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3488 - accuracy: 0.8600\n",
      "Epoch 00033: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8975 - val_loss: 0.3322 - val_accuracy: 0.8539\n",
      "Epoch 34/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.1051 - accuracy: 0.9800\n",
      "Epoch 00034: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8925 - val_loss: 0.3672 - val_accuracy: 0.8539\n",
      "Epoch 35/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3227 - accuracy: 0.8800\n",
      "Epoch 00035: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.8950 - val_loss: 0.3408 - val_accuracy: 0.8539\n",
      "Epoch 36/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2030 - accuracy: 0.9000\n",
      "Epoch 00036: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.8775 - val_loss: 0.3646 - val_accuracy: 0.8652\n",
      "Epoch 37/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2695 - accuracy: 0.9000\n",
      "Epoch 00037: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8913 - val_loss: 0.3907 - val_accuracy: 0.8427\n",
      "Epoch 38/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2273 - accuracy: 0.9000\n",
      "Epoch 00038: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8813 - val_loss: 0.3563 - val_accuracy: 0.8652\n",
      "Epoch 39/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.2638 - accuracy: 0.9000\n",
      "Epoch 00039: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8913 - val_loss: 0.4516 - val_accuracy: 0.8202\n",
      "Epoch 40/40\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.3313 - accuracy: 0.8400\n",
      "Epoch 00040: val_loss did not improve from 0.29784\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.9000 - val_loss: 0.3308 - val_accuracy: 0.8764\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import save_model\n",
    "\n",
    "opt = Adam(learning_rate=0.0009, amsgrad=True)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('neural_network_checkpoint_training.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "model.fit(x=X_train, y=y_train,\n",
    "        epochs=40,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=50,\n",
    "        shuffle=True,\n",
    "        callbacks=[tensorboard, checkpoint],\n",
    "        verbose=1)\n",
    "\n",
    "save_model(checkpoint.model, 'neural_network_latest_saved.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0479,
     "end_time": "2021-01-14T01:47:10.255319",
     "exception": false,
     "start_time": "2021-01-14T01:47:10.207419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The final prediction and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T01:47:10.363580Z",
     "iopub.status.busy": "2021-01-14T01:47:10.362912Z",
     "iopub.status.idle": "2021-01-14T01:47:10.955896Z",
     "shell.execute_reply": "2021-01-14T01:47:10.956370Z"
    },
    "papermill": {
     "duration": 0.652833,
     "end_time": "2021-01-14T01:47:10.956500",
     "exception": false,
     "start_time": "2021-01-14T01:47:10.303667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "clf = load_model('neural_network_checkpoint_training.h5')\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "test_prediction = clf.predict(X_test.values).flatten()\n",
    "\n",
    "test_prediction[test_prediction <= threshold] = 0\n",
    "test_prediction[test_prediction > threshold] = 1\n",
    "\n",
    "test_prediction = pd.DataFrame(test_prediction, columns=['Survived'], dtype=np.int)\n",
    "\n",
    "prediction = pd.concat([test_ids, test_prediction], axis=1)\n",
    "\n",
    "prediction.to_csv('submission_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048684,
     "end_time": "2021-01-14T01:47:11.053779",
     "exception": false,
     "start_time": "2021-01-14T01:47:11.005095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "With this method we achieve a final score of 78.9% on the test dataset according to Kaggle, which puts this classifier in the top 11%. Even though the dataset is considerably small, the MLP Neural Network still manages to produce a satisfying accuracy, which can be further improved with more detailed feature engineering of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 16.935129,
   "end_time": "2021-01-14T01:47:11.208705",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-14T01:46:54.273576",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
